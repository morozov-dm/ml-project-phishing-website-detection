# -*- coding: utf-8 -*-
"""[Eng версия] Итоговая_работа_(Финальная версия)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F3dj6cn7cPpceNiBj1Su9n62qtvcQPaQ

# **Stage 1: Data Loading and Preprocessing**

Environment Setup
"""

# Core libraries
import pandas as pd
import numpy as np
import json

# Data preparation libraries
from sklearn.model_selection import train_test_split

# Data splitting constants
RANDOM_STATE = 42
TEST_SIZE = 0.2

"""1.1 Loading Raw Data"""

from google.colab import drive
from scipy.io import arff

drive.mount('/content/drive/')

DATA_PATH = '/content/drive/MyDrive/phishing.arff'
data_arff, meta = arff.loadarff(DATA_PATH)

df_raw = pd.DataFrame(data_arff)

print(f"Data successfully loaded from '{DATA_PATH}'. Shape: {df_raw.shape}")
print("First 5 rows:")
display(df_raw.head())

"""1.2 Decoding, Normalization, and Duplicate Removal"""

# Decode and normalize target variable
df_clean = df_raw.copy()
for col in df_clean.columns:
    if df_clean[col].dtype == 'object':
        df_clean[col] = df_clean[col].str.decode('utf-8').astype(int)
df_clean['Result'] = df_clean['Result'].map({-1: 0, 1: 1})
print(f"\nTarget variable 'Result' normalized to {df_clean['Result'].unique()}")

# Remove duplicates
dup_rows_count_total = df_clean.duplicated().sum()
print(f"Found {dup_rows_count_total} duplicate rows.")
if dup_rows_count_total > 0:
    df_clean.drop_duplicates(inplace=True)
    print(f"Duplicates removed. New shape of df_clean: {df_clean.shape}")
    print("Class balance in df_clean after duplicate removal:")
    print(df_clean['Result'].value_counts(normalize=True))

"""1.3 Data Splitting"""

# Split df_clean into training and test sets
df_train, df_test = train_test_split(
    df_clean,
    test_size=TEST_SIZE,
    random_state=RANDOM_STATE,
    stratify=df_clean['Result']
)

print(f"\nSplit completed:")
print(f"  df_train (for analysis and training): {df_train.shape}")
print(f"  df_test (hold-out test set):        {df_test.shape}")

# Validate training data
print("\nValidating data types and missing values:")
df_train.info()

print("\nValidating feature value ranges:")
feature_cols = df_train.drop('Result', axis=1).columns
allowed_values = {-1, 0, 1}

invalid_cols = []
for col in feature_cols:
    unique_vals = set(df_train[col].unique())
    if not unique_vals.issubset(allowed_values):
        invalid_cols.append(col)

if not invalid_cols:
    print("All features contain only values from {-1, 0, 1}.")
else:
    print(f"ERROR: Columns with invalid values: {invalid_cols}")

# Validate target variable separately
if not set(df_train['Result'].unique()).issubset({0, 1}):
    print(f"ERROR: 'Result' contains invalid values!")
else:
    print(f"'Result' contains only values {sorted(df_train['Result'].unique())}.")

"""# **Stage 2: Data Analysis**

Environment Setup
"""

# Libraries for visualization
import plotly.express as px
import plotly.graph_objects as go

# Libraries for feature analysis
from sklearn.feature_selection import mutual_info_classif
from scipy.stats.contingency import association

# Constants
THRESHOLD = 0.8
RANDOM_STATE = 42

"""2.1 Identifying Low-Variance and Duplicate Features"""

# Remove redundant features
# Features with only 1 unique value (Zero Variance)
useless_cols_zv = df_train[feature_cols].columns[df_train[feature_cols].nunique() == 1]
print(f"ZV (Zero Variance): Found {len(useless_cols_zv)} features.")
print(f"   -> {list(useless_cols_zv)}")

# Features where the most frequent value accounts for > 99% of data (Near-Zero Variance)
useless_cols_nzv = []
for col in feature_cols:
    top_freq = df_train[col].value_counts(normalize=True).iloc[0]
    if top_freq > 0.99:
        useless_cols_nzv.append(col)

print(f"NZV (Near-Zero Variance): Found {len(useless_cols_nzv)} features.")
print(f"   -> {list(useless_cols_nzv)}")

# Remove duplicate features (columns)
df_train_T = df_train[feature_cols].T
duplicated_cols = df_train_T[df_train_T.duplicated()].index
useless_cols_dup = list(duplicated_cols)

print(f"Duplicate columns: Found {len(useless_cols_dup)} features.")
print(f"   -> {list(useless_cols_dup)}")

# Save artifact
useless_features = set(useless_cols_zv) | set(useless_cols_nzv) | set(useless_cols_dup)
useless_features_list = sorted(list(useless_features))

ARTIFACT_PATH = 'useless_features.json'
with open(ARTIFACT_PATH, 'w') as f:
    json.dump(useless_features_list, f, indent=4)

print(f"Total {len(useless_features_list)} low-quality features identified.")
print(f"List saved to '{ARTIFACT_PATH}'.")

print(f"Final shape of df_train: {df_train.shape}")

"""2.2 Visual Analysis of Feature-Target Relationships

"""

# Create a helper column for plotting
df_train['Result_str'] = df_train['Result'].map({0: 'Legitimate (0)', 1: 'Phishing (1)'})
features_to_analyze = df_train.drop(columns=['Result', 'Result_str']).columns

print(f"Below are bar plots for {len(features_to_analyze)} features:")

# Create plot
for col in features_to_analyze:
    fig = px.histogram(
        df_train,
        x=col,
        color='Result_str',
        barmode='group',
        histnorm='percent',
        title=f"Pattern: {col} vs. Result (Phishing/Legitimate)",
        color_discrete_map={'Legitimate (0)': '#00A08B', 'Phishing (1)': '#D62728'},
    )

    fig.update_layout(
        yaxis_title="Percentage (%)",
        xaxis_title=f"Value of feature '{col}'",
        legend_title="Result"
    )
    fig.update_xaxes(type='category')

    fig.show()

# Drop the helper column after plotting
df_train = df_train.drop(columns=['Result_str'])

"""2.3 Feature Importance: Mutual Information"""

# Prepare training data
X_train = df_train.drop('Result', axis=1)
y_train = df_train['Result']

print(f"X_train (features): {X_train.shape}")
print(f"y_train (target): {y_train.shape}")

# Feature importance (Feature vs. Target)
print("Feature Importance (Mutual Information)")

# Calculate Mutual Information (MI)
mi_scores = mutual_info_classif(
    X_train,
    y_train,
    discrete_features=True, #
    random_state=RANDOM_STATE
)

mi_scores_df = pd.DataFrame({'Feature': X_train.columns, 'MI_Score': mi_scores})
mi_scores_df = mi_scores_df.sort_values(by='MI_Score', ascending=False)

print("Feature ranking by Mutual Information:")
print(mi_scores_df.head(20).to_markdown(index=False))

# Visualize MI
print("\nFeature Importance Plot (Mutual Information):")
fig_mi = px.bar(
    mi_scores_df.head(20),
    x='MI_Score',
    y='Feature',
    orientation='h',
    title="Feature Importance (Mutual Information)",
    labels={"MI_Score"}
)
fig_mi.update_layout(yaxis={'categoryorder':'total ascending'})
fig_mi.show()

"""2.4 Feature Correlation Analysis: Cramér's V"""

# Calculate full N x N Cramér's V matrix
n_cols = len(X_train.columns)

cramer_matrix = pd.DataFrame(
    np.zeros((n_cols, n_cols)),
    index=X_train.columns,
    columns=X_train.columns
)

crosstab_cache = {}
for col1 in X_train.columns:
    for col2 in X_train.columns:
        if col1 == col2:
            cramer_matrix.loc[col1, col2] = 1.0
            continue

        pair_key = tuple(sorted((col1, col2)))

        if pair_key in crosstab_cache:
            v_cramer = crosstab_cache[pair_key]
        else:
            contingency_table = pd.crosstab(X_train[col1], X_train[col2])
            v_cramer = association(contingency_table, method='cramer')
            crosstab_cache[pair_key] = v_cramer

        cramer_matrix.loc[col1, col2] = v_cramer

# Visualization (Heatmap)
print("Cramér's V Heatmap (Multicollinearity):")
fig_heatmap = px.imshow(
    cramer_matrix,
    title="Cramér's V Heatmap (Multicollinearity):",
    color_continuous_scale='Viridis',
    labels=dict(color="Cramér's V (1 = 'Perfect Association')")
)
fig_heatmap.update_layout(
    coloraxis_colorbar=dict(
        title="Cramér's V",
        x=0.7
    )
)
fig_heatmap.show()

"""2.5 Feature Selection Based on Cramér's V and Mutual Information"""

# Initialize list for dropping collinear features
features_to_drop_collinear = set()

# Prepare MI score dictionary
mi_scores_dict = mi_scores_df.set_index('Feature')['MI_Score'].to_dict()
print(f"Searching for feature pairs with Cramér's V > {THRESHOLD}:")

# Iterate through all unique feature pairs to compute Cramér's V
n_cols = len(X_train.columns)
for i in range(n_cols):
    for j in range(i + 1, n_cols):
        col1 = X_train.columns[i]
        col2 = X_train.columns[j]

        # Get Cramér's V value for the current pair from the matrix
        v_value = cramer_matrix.loc[col1, col2]

        # Check for strong collinearity
        if v_value > THRESHOLD:
            print(f"[Found pair] '{col1}' and '{col2}' (V = {v_value:.3f})")

            # Compare feature importance (MI) in the collinear pair
            mi1 = mi_scores_dict.get(col1, 0)
            mi2 = mi_scores_dict.get(col2, 0)

            # Remove the feature with lower MI score, as it is less informative
            if mi1 > mi2:
                print(f"  -> DECISION: Drop '{col2}' (MI: {mi2:.4f}), as '{col1}' has higher MI ({mi1:.4f})")
                features_to_drop_collinear.add(col2)
            else:
                print(f"  -> DECISION: Drop '{col1}' (MI: {mi1:.4f}), as '{col2}' has higher MI ({mi2:.4f})")
                features_to_drop_collinear.add(col1)

# Save artifact
ARTIFACT_PATH_2 = 'collinear_features.json'
features_to_drop_collinear_list = sorted(list(features_to_drop_collinear))

with open(ARTIFACT_PATH_2, 'w') as f:
    json.dump(features_to_drop_collinear_list, f, indent=4)

print(f"\nIn total, {len(features_to_drop_collinear_list)} collinear features were identified for removal.")
print(f"List saved to '{ARTIFACT_PATH_2}'.")
print(f"   -> {features_to_drop_collinear_list}")

"""# **Stage 3: Data Preprocessing**

Environment Setup
"""

# Core libraries
import pandas as pd
import numpy as np

# Libraries for artifact loading
import json
import joblib

# For creating scaler
from sklearn.preprocessing import StandardScaler

# Constants
VALIDATION_SIZE = 0.25
RANDOM_STATE = 42

"""3.1 Loading Artifacts"""

# Load artifacts
ARTIFACT_PATH_USELESS = 'useless_features.json'
ARTIFACT_PATH_COLLINEAR = 'collinear_features.json'

useless_features_list = []
collinear_features_list = []

# Load list of low-variance features
try:
    with open(ARTIFACT_PATH_USELESS, 'r') as f:
        useless_features_list = json.load(f)
    print(f"Loaded {len(useless_features_list)} low-variance features from '{ARTIFACT_PATH_USELESS}'.")
    print(f"   -> {useless_features_list}")
except FileNotFoundError:
    print(f"File '{ARTIFACT_PATH_USELESS}' not found. Low-variance features list will be empty.")
except json.JSONDecodeError:
    print(f"Error reading '{ARTIFACT_PATH_USELESS}'. Low-variance features list will be empty.")

# Load list of collinear features
try:
    with open(ARTIFACT_PATH_COLLINEAR, 'r') as f:
        collinear_features_list = json.load(f)
    print(f"Loaded {len(collinear_features_list)} collinear features from '{ARTIFACT_PATH_COLLINEAR}'.")
    print(f"   -> {collinear_features_list}")
except FileNotFoundError:
    print(f"File '{ARTIFACT_PATH_COLLINEAR}' not found. Collinear features list will be empty.")
except json.JSONDecodeError:
    print(f"Error reading '{ARTIFACT_PATH_COLLINEAR}'. Collinear features list will be empty.")

# Combine lists of features to drop
features_to_drop = sorted(list(set(useless_features_list + collinear_features_list)))
print(f"\nTotal features to be dropped ({len(features_to_drop)}): {features_to_drop}")
print("-" * 40)

"""3.2 Data Splitting and Feature Removal"""

# Target variable
TARGET = 'Result'

# Split training data
X_train = df_train.drop(TARGET, axis=1)
y_train = df_train[TARGET]

# Split test data
X_test = df_test.drop(TARGET, axis=1)
y_test = df_test[TARGET]

# Clean data
# Print shapes BEFORE removal
print(f"Shapes BEFORE removal: X_train={X_train.shape}, X_test={X_test.shape}")
print(f"Number of features BEFORE removal: {X_train.shape[1]}")

# Drop features, ignoring missing ones
X_train = X_train.drop(columns=features_to_drop, errors='ignore')
X_test = X_test.drop(columns=features_to_drop, errors='ignore')

# Print shapes AFTER removal
print(f"\nShapes AFTER removal: X_train={X_train.shape}, X_test={X_test.shape}")
print(f"Number of features AFTER removal: {X_train.shape[1]}")

# Additional split of X_train/y_train into folds for Optuna
X_train_fold, X_val_fold, y_train_fold, y_val_fold = train_test_split(
    X_train,
    y_train,
    test_size=VALIDATION_SIZE,
    random_state=RANDOM_STATE,
    stratify=y_train
)

print("\nSplit for Optuna:")
print(f"  X_train_fold (for training in trial): {X_train_fold.shape}")
print(f"  X_val_fold (for validation in trial): {X_val_fold.shape}")
print(f"  y_train_fold (for training in trial): {y_train_fold.shape}")
print(f"  y_val_fold (for validation in trial): {y_val_fold.shape}")

"""3.3 Feature Scaling (StandardScaler)"""

# Create StandardScaler
scaler = StandardScaler()

# Fit scaler ONLY on X_train_fold
scaler.fit(X_train_fold)

# Scale X_train_fold and X_val_fold
X_train_fold_scaled = scaler.transform(X_train_fold)
X_val_fold_scaled = scaler.transform(X_val_fold)

# Scale entire X_train for final training (AFTER Optuna)
X_train_scaled = scaler.transform(X_train)

# Scale X_test with the same scaler (for final evaluation)
X_test_scaled = scaler.transform(X_test)

# Save fitted scaler
SCALER_PATH = 'scaler.pkl'
joblib.dump(scaler, SCALER_PATH)
print(f"Fitted scaler (trained on X_train_fold) saved to: '{SCALER_PATH}'")

# Verify scaling on folds
print("\nVerification of scaling on folds:")
print(f"Mean of X_train_fold_scaled (expected ~0):  {X_train_fold_scaled.mean():.20f}")
print(f"Std of X_train_fold_scaled (expected ~1):   {X_train_fold_scaled.std():.20f}")
print(f"Mean of X_val_fold_scaled (expected not 0): {X_val_fold_scaled.mean():.20f}")
print(f"Std of X_val_fold_scaled (expected not 1):  {X_val_fold_scaled.std():.20f}")

# Verify scaling on full train and test
print("\nVerification of scaling on full train and test:")
print(f"Mean of X_train_scaled: {X_train_scaled.mean():.20f}")
print(f"Std of X_train_scaled:  {X_train_scaled.std():.20f}")
print(f"Mean of X_test_scaled:  {X_test_scaled.mean():.20f}")
print(f"Std of X_test_scaled:   {X_test_scaled.std():.20f}")

"""# **Stage 4: Model Building and Training**

Environment Setup
"""

# Install libraries
!pip install mlflow optuna xgboost optuna-integration[mlflow]

# Core libraries
import pandas as pd
import numpy as np
import json
import time
import os

# Machine learning libraries
import xgboost as xgb
import tensorflow as tf
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2
from sklearn.metrics import roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, classification_report
from tensorflow.keras.callbacks import EarlyStopping

# Optuna libraries
import optuna
import optuna.integration.mlflow
from optuna.integration import TFKerasPruningCallback

# MLflow libraries
import mlflow
import mlflow.sklearn
import mlflow.xgboost
import mlflow.tensorflow
from mlflow.tracking import MlflowClient
from mlflow.exceptions import MlflowException

# Constants
RANDOM_STATE = 42

# Suppress Optuna logs
optuna.logging.set_verbosity(optuna.logging.WARNING)

# Setup MLflow callback for Optuna
mlflow_callback = optuna.integration.MLflowCallback(
    tracking_uri=mlflow.get_tracking_uri(),
    metric_name='roc_auc_val',
    create_experiment=True
)

print("\n\nEnvironment successfully set up.")

"""**4.1 Model №1: Logistic Regression**"""

# Log to MLflow
with mlflow.start_run(run_name="LogisticRegression_Tuned"):

    # Define parameter grid for search
    param_grid_lr = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}

    # Initialize GridSearchCV
    grid_lr = GridSearchCV(
        LogisticRegression(solver='lbfgs', max_iter=1000, random_state=RANDOM_STATE),
        param_grid_lr,
        cv=5,
        scoring='roc_auc',
        n_jobs=-1
    )

    # Measure training start time
    start_time = time.time()

    # Fit GridSearchCV on scaled training data
    grid_lr.fit(X_train_scaled, y_train)

    # Calculate total training time
    train_time = time.time() - start_time

    # Get best model
    model_lr = grid_lr.best_estimator_

    # Log parameters
    mlflow.log_param("model_type", "LogisticRegression")
    mlflow.log_params(grid_lr.best_params_)
    mlflow.log_metric("cv_roc_auc", grid_lr.best_score_)
    mlflow.log_metric("train_time_sec", train_time)

    # Evaluate on training set and log metrics
    y_train_pred_proba = model_lr.predict_proba(X_train_scaled)[:, 1]
    train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)
    mlflow.log_metric("train_roc_auc", train_roc_auc)

    y_train_pred_class = model_lr.predict(X_train_scaled)
    train_precision = precision_score(y_train, y_train_pred_class)
    train_recall = recall_score(y_train, y_train_pred_class)
    train_f1 = f1_score(y_train, y_train_pred_class)
    mlflow.log_metric("train_precision", train_precision)
    mlflow.log_metric("train_recall", train_recall)
    mlflow.log_metric("train_f1", train_f1)

    # Evaluate on test set and log metrics
    y_test_pred_proba = model_lr.predict_proba(X_test_scaled)[:, 1]
    test_roc_auc = roc_auc_score(y_test, y_test_pred_proba)
    mlflow.log_metric("test_roc_auc", test_roc_auc)

    y_test_pred_class = model_lr.predict(X_test_scaled)
    test_precision = precision_score(y_test, y_test_pred_class)
    test_recall = recall_score(y_test, y_test_pred_class)
    test_f1 = f1_score(y_test, y_test_pred_class)
    mlflow.log_metric("test_precision", test_precision)
    mlflow.log_metric("test_recall", test_recall)
    mlflow.log_metric("test_f1", test_f1)

    # Confusion matrix: calculate and log as artifact
    cm = confusion_matrix(y_test, y_test_pred_class)
    print("\nConfusion Matrix (Test, LogReg):")
    print(cm)

    cm_file_path = "confusion_matrix_test_lr.json"
    cm_list = cm.tolist()
    with open(cm_file_path, 'w') as f:
        json.dump(cm_list, f, indent=4)
    mlflow.log_artifact(cm_file_path)
    os.remove(cm_file_path)

    # Prepare example input data and signature
    current_feature_names = list(X_train.columns)
    input_example = pd.DataFrame(X_train_scaled[:5], columns=current_feature_names)
    signature = mlflow.models.infer_signature(X_train_scaled, model_lr.predict_proba(X_train_scaled))

    # Log best model to MLflow
    mlflow.sklearn.log_model(
        sk_model=model_lr,
        name="logistic_regression_model",
        signature=signature,
        input_example=input_example,
        registered_model_name="LogisticRegression_Phishing_Final"
    )

    # Print results
    print(f"\nFinal LogisticRegression model trained, additional metrics evaluated and logged.")
    print(f"Best parameter C (with CV):  {grid_lr.best_params_['C']}")
    print(f"ROC AUC on cross-validation: {grid_lr.best_score_:.4f}")
    print(f"ROC AUC on training set:     {train_roc_auc:.4f}")
    print(f"Precision on test:           {test_precision:.4f}")
    print(f"Recall on test:              {test_recall:.4f}")
    print(f"F1-score on test:            {test_f1:.4f}")
    print(f"ROC AUC on test set:         {test_roc_auc:.4f}")
    print(f"Training time (with CV):     {train_time:.2f} sec.")
    print(f"Model registered in MLflow as: LogisticRegression_Phishing_Final\n")

"""**4.2 Model №2: XGBoost**

4.2.1 Defining the Objective Function for Optuna
"""

# Objective function for hyperparameter optimization of XGBoost using Optuna
def objective_xgboost(trial):
    """
    Objective function for hyperparameter optimization of XGBoost using Optuna.
    """
    params = {
        'objective': 'binary:logistic',
        'eval_metric': 'auc',
        'random_state': RANDOM_STATE,
        # Hyperparameters for optimization
        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
        'gamma': trial.suggest_float('gamma', 0, 0.5),
        'lambda': trial.suggest_float('lambda', 1e-8, 1.0, log=True),
        'alpha': trial.suggest_float('alpha', 1e-8, 1.0, log=True),
        'tree_method': 'hist'
    }

    # Create model
    model = xgb.XGBClassifier(**params)

    # Train model on training fold using validation set for early stopping
    model.fit(
        X_train_fold_scaled, y_train_fold,
        eval_set=[(X_val_fold_scaled, y_val_fold)],
        verbose=False
    )

    # Get ROC AUC metric on validation fold for Optuna Pruner
    roc_auc = model.evals_result()['validation_0']['auc'][-1]
    # Return metric for Optuna
    return roc_auc


print("Objective function 'objective_xgboost' for Optuna defined.")

"""4.2.2 Hyperparameter Optimization with Optuna"""

# Create Optuna study
study = optuna.create_study(
    direction='maximize',
    pruner=optuna.pruners.MedianPruner(n_warmup_steps=5)
)

# Run optimization
N_TRIALS = 50
study.optimize(
    objective_xgboost,
    n_trials=N_TRIALS,
    callbacks=[mlflow_callback],
    gc_after_trial=True
)

# Print optimization results
print(f"\nOptimization completed after {N_TRIALS} trials.")
print(f"Best value (Validation ROC AUC): {study.best_value:.6f}")
print("Best hyperparameters:")
best_params_xgb = study.best_params
for key, value in best_params_xgb.items():
    print(f"  {key}: {value}")

# Add fixed parameters for the final model
best_params_xgb['objective'] = 'binary:logistic'
best_params_xgb['eval_metric'] = 'auc'
best_params_xgb['random_state'] = RANDOM_STATE
best_params_xgb['tree_method'] = 'hist'

print("\nBest parameters saved to best_params_xgb.")

"""4.2.3 Training the Final XGBoost Model"""

# Use best parameters found by Optuna
final_model_xgb = xgb.XGBClassifier(**best_params_xgb)

# Log final training in MLflow
with mlflow.start_run(run_name="XGBoost_Final_Model"):

    # Measure training start time
    start_time = time.time()

    # Train final model on full scaled training set
    final_model_xgb.fit(
        X_train_scaled, y_train,
        eval_set=[(X_train_scaled, y_train), (X_test_scaled, y_test)],
        verbose=False
    )

    # Calculate total training time
    train_time = time.time() - start_time

    # Log parameters
    mlflow.log_param("model_type", "XGBoost")
    mlflow.log_params(best_params_xgb)
    mlflow.log_metric("train_time_sec", train_time)

    # Evaluate on training set and log metrics
    y_train_pred_proba = final_model_xgb.predict_proba(X_train_scaled)[:, 1]
    train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)
    mlflow.log_metric("train_roc_auc", train_roc_auc)

    y_train_pred_class = final_model_xgb.predict(X_train_scaled)
    train_precision = precision_score(y_train, y_train_pred_class)
    train_recall = recall_score(y_train, y_train_pred_class)
    train_f1 = f1_score(y_train, y_train_pred_class)

    mlflow.log_metric("train_precision", precision_score(y_train, y_train_pred_class))
    mlflow.log_metric("train_recall", recall_score(y_train, y_train_pred_class))
    mlflow.log_metric("train_f1", f1_score(y_train, y_train_pred_class))

    # Evaluate on test set and log metrics
    y_test_pred_proba = final_model_xgb.predict_proba(X_test_scaled)[:, 1]
    test_roc_auc = roc_auc_score(y_test, y_test_pred_proba)
    mlflow.log_metric("test_roc_auc", test_roc_auc)

    y_test_pred_class = final_model_xgb.predict(X_test_scaled)
    test_precision = precision_score(y_test, y_test_pred_class)
    test_recall = recall_score(y_test, y_test_pred_class)
    test_f1 = f1_score(y_test, y_test_pred_class)

    mlflow.log_metric("test_precision", test_precision)
    mlflow.log_metric("test_recall", test_recall)
    mlflow.log_metric("test_f1", test_f1)

    # Confusion matrix: calculate and log as artifact
    cm = confusion_matrix(y_test, y_test_pred_class)
    print("\nConfusion Matrix (Test, XGB):")
    print(cm)

    cm_file_path = "confusion_matrix_test_xgb.json"
    cm_list = cm.tolist()
    with open(cm_file_path, 'w') as f:
        json.dump(cm_list, f, indent=4)
    mlflow.log_artifact(cm_file_path)
    print(f"Confusion matrix (Test, XGB) saved as artifact: {cm_file_path}")
    os.remove(cm_file_path)

    # Prepare example input data and signature
    current_feature_names = list(X_train.columns)
    input_example = pd.DataFrame(X_train_scaled[:5], columns=current_feature_names)
    signature = mlflow.models.infer_signature(X_train_scaled, final_model_xgb.predict_proba(X_train_scaled))

    # Log final model in MLflow
    mlflow.xgboost.log_model(
        xgb_model=final_model_xgb,
        name="xgboost_model",
        signature=signature,
        input_example=input_example,
        registered_model_name="XGBoost_Phishing_Final",
    )

    # Print results
    print(f"\nFinal XGBoost model trained, additional metrics evaluated and logged.")
    print(f"ROC AUC on training set: {train_roc_auc:.4f}")
    print(f"ROC AUC on test set:     {test_roc_auc:.4f}")
    print(f"Precision on test:       {test_precision:.4f}")
    print(f"Recall on test:          {test_recall:.4f}")
    print(f"F1-score on test:        {test_f1:.4f}")
    print(f"Training time:           {train_time:.2f} sec.")
    print(f"Model registered in MLflow as: XGBoost_Phishing_Final\n")

"""**4.3 Model #3: Neural Network**

4.3.1 Defining the Neural Network Architecture (Keras)
"""

# Function to create a Keras model with customizable parameters
def build_keras_model(input_shape, n_layers=1, n_neurons=32, activation='relu', learning_rate=0.001, dropout_rate=0.0, l2_reg=0.0):
    """
    Creates a compiled Keras model with an explicit input layer.
    """
    # Define the input layer
    input_layer = Input(shape=(input_shape,))

    # First hidden layer
    x = Dense(n_neurons, activation=activation, kernel_regularizer=l2(l2_reg))(input_layer)
    if dropout_rate > 0:
        x = Dropout(dropout_rate)(x)

    # Add additional hidden layers
    for _ in range(n_layers - 1):
        x = Dense(n_neurons, activation=activation, kernel_regularizer=l2(l2_reg))(x)
        if dropout_rate > 0:
            x = Dropout(dropout_rate)(x)

    # Output layer (for binary classification)
    output_layer = Dense(1, activation='sigmoid')(x)

    # Create model by specifying input and output layers
    model = Model(inputs=input_layer, outputs=output_layer)

    # Define optimizer
    optimizer = Adam(learning_rate=learning_rate)

    # Compile model
    model.compile(optimizer=optimizer,
                  loss='binary_crossentropy',
                  metrics=['AUC'])

    return model

print("Function 'build_keras_model' successfully configured.")

"""4.3.2 Defining the Objective Function for Optuna"""

# Objective function for Optuna
def objective_keras(trial):
    """
    Objective function for hyperparameter optimization of Keras model using Optuna.
    """
    # Suggest hyperparameters for Optuna
    n_layers = trial.suggest_int('n_layers', 1, 3)
    n_neurons = trial.suggest_int('n_neurons', 32, 128, step=32)
    activation = trial.suggest_categorical('activation', ['relu', 'tanh'])
    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)
    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5, step=0.1)
    l2_reg = trial.suggest_float('l2_reg', 1e-6, 1e-3, log=True)

    # Create Keras model with suggested parameters
    model = build_keras_model(
        input_shape=X_train_fold_scaled.shape[1],
        n_layers=n_layers,
        n_neurons=n_neurons,
        activation=activation,
        learning_rate=learning_rate,
        dropout_rate=dropout_rate,
        l2_reg=l2_reg
    )

    # Define callbacks for Keras
    # Early Stopping to stop training if validation AUC does not improve
    early_stopping = EarlyStopping(
        monitor='val_AUC',
        patience=10,
        mode='max',
        restore_best_weights=True
    )

    # Optuna Pruning Callback to prune unpromising trials early
    pruning_callback = TFKerasPruningCallback(trial, 'val_AUC')

    # List of callbacks to pass to model.fit
    callbacks = [early_stopping, pruning_callback]

    # Train model on training fold with validation set
    history = model.fit(
        X_train_fold_scaled,
        y_train_fold,
        epochs=100,
        batch_size=32,
        validation_data=(X_val_fold_scaled, y_val_fold),
        callbacks=callbacks,
        verbose=0
    )

    # Evaluate model on validation set after training
    loss, auc = model.evaluate(X_val_fold_scaled, y_val_fold, verbose=0)
    # Return metric for Optuna
    return auc

print("Objective function 'objective_keras' for Optuna defined.")

"""4.3.3 Hyperparameter Optimization with Optuna"""

# Create Optuna study
study_keras = optuna.create_study(
    direction='maximize',
    pruner=optuna.pruners.MedianPruner(n_warmup_steps=5)
)

# Run optimization with objective_keras
N_TRIALS_KERAS = 50
study_keras.optimize(
    objective_keras,
    n_trials=N_TRIALS_KERAS,
    callbacks=[mlflow_callback],
    gc_after_trial=True
)

# Print optimization results
print(f"\nKeras model optimization completed after {N_TRIALS_KERAS} trials.")
print(f"Best value (Validation ROC AUC): {study_keras.best_value:.6f}")
print("Best hyperparameters for Keras model:")
best_params_keras = study_keras.best_params
for key, value in best_params_keras.items():
    print(f"  {key}: {value}")

print("\nBest parameters saved to best_params_keras.")

"""4.3.4 Training the Final Keras Model"""

# Use best parameters found by Optuna
final_model_keras = build_keras_model(
    input_shape=X_train_scaled.shape[1],
    n_layers=best_params_keras['n_layers'],
    n_neurons=best_params_keras['n_neurons'],
    activation=best_params_keras['activation'],
    learning_rate=best_params_keras['learning_rate'],
    dropout_rate=best_params_keras['dropout_rate'],
    l2_reg=best_params_keras['l2_reg']
)

print("\nFinal Keras model built with best parameters.")
final_model_keras.summary()

# Log final training in MLflow
with mlflow.start_run(run_name="Keras_Final_Model"):

    # Measure training start time
    start_time = time.time()

    # Train final model on full scaled training set
    history = final_model_keras.fit(
        X_train_scaled, y_train,
        epochs=100,
        batch_size=32,
        validation_data=(X_test_scaled, y_test),
        verbose=0
    )

    # Calculate total training time
    train_time = time.time() - start_time

    # Log parameters
    mlflow.log_param("model_type", "Keras_NN")
    mlflow.log_params(best_params_keras)
    mlflow.log_metric("train_time_sec", train_time)

    # Evaluate on training set and log metrics
    loss_train, train_roc_auc = final_model_keras.evaluate(X_train_scaled, y_train, verbose=0)
    mlflow.log_metric("train_roc_auc", train_roc_auc)

    y_train_pred_class = (final_model_keras.predict(X_train_scaled) > 0.5).astype("int32")
    train_precision = precision_score(y_train, y_train_pred_class)
    train_recall = recall_score(y_train, y_train_pred_class)
    train_f1 = f1_score(y_train, y_train_pred_class)

    mlflow.log_metric("train_precision", train_precision)
    mlflow.log_metric("train_recall", train_recall)
    mlflow.log_metric("train_f1", train_f1)

    # Evaluate on test set and log metrics
    loss_test, test_roc_auc = final_model_keras.evaluate(X_test_scaled, y_test, verbose=0)
    mlflow.log_metric("test_roc_auc", test_roc_auc)

    y_test_pred_class = (final_model_keras.predict(X_test_scaled) > 0.5).astype("int32")
    test_precision = precision_score(y_test, y_test_pred_class)
    test_recall = recall_score(y_test, y_test_pred_class)
    test_f1 = f1_score(y_test, y_test_pred_class)

    mlflow.log_metric("test_precision", test_precision)
    mlflow.log_metric("test_recall", test_recall)
    mlflow.log_metric("test_f1", test_f1)

    # Confusion matrix: calculate and log as artifact
    cm = confusion_matrix(y_test, y_test_pred_class)
    print("\nConfusion Matrix (Test, Keras):")
    print(cm)

    cm_file_path = "confusion_matrix_test_keras.json"
    cm_list = cm.tolist()
    with open(cm_file_path, 'w') as f:
        json.dump(cm_list, f, indent=4)
    mlflow.log_artifact(cm_file_path)
    print(f"Confusion matrix (Test, Keras) saved as artifact: {cm_file_path}")
    os.remove(cm_file_path)

    # Prepare example input data and signature
    current_feature_names = list(X_train.columns)
    input_example = pd.DataFrame(X_train_scaled[:5], columns=current_feature_names)
    signature = mlflow.models.infer_signature(X_train_scaled, final_model_keras.predict(input_example))

    # Log final model in MLflow
    mlflow.tensorflow.log_model(
        model=final_model_keras,
        name="keras_model",
        signature=signature,
        input_example=input_example,
        registered_model_name="Keras_Phishing_Final"
    )

    # Print results
    print(f"\nFinal Keras model trained, additional metrics evaluated and logged.")
    print(f"ROC AUC on training set: {train_roc_auc:.4f}")
    print(f"ROC AUC on test set:     {test_roc_auc:.4f}")
    print(f"Precision on test:       {test_precision:.4f}")
    print(f"Recall on test:          {test_recall:.4f}")
    print(f"F1-score on test:        {test_f1:.4f}")
    print(f"Training time:           {train_time:.2f} sec.")
    print(f"Model registered in MLflow as: Keras_Phishing_Final\n")

"""# **Stage 5: Model Comparison and Evaluation**

Environment Setup
"""

# Core libraries
import numpy as np
import pandas as pd

# Visualization libraries
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.graph_objects as go

# Libraries for loading and evaluating models
import json
import os
import joblib
import tensorflow as tf
from tensorflow.keras.models import load_model
from sklearn.metrics import roc_curve, auc

# MLflow libraries
import mlflow
from mlflow.exceptions import MlflowException
from mlflow.tracking import MlflowClient

"""5.1 Loading Models, Metrics, and Confusion Matrices from MLflow"""

# Specify path to MLflow
mlflow.set_tracking_uri("mlruns")

# Create MLflow client
client = MlflowClient()

# Dictionaries to store loaded data
loaded_models = {}
model_metrics = {}
confusion_matrices = {}

# Registered model names and loading info
model_info = {
    "LogisticRegression_Phishing_Final": {"artifact_name": "logistic_regression_model", "is_keras": False, "cm_artifact_name": "confusion_matrix_test_lr.json"},
    "XGBoost_Phishing_Final": {"artifact_name": "xgboost_model", "is_keras": False, "cm_artifact_name": "confusion_matrix_test_xgb.json"},
    "Keras_Phishing_Final": {"artifact_name": "keras_model", "is_keras": True, "cm_artifact_name": "confusion_matrix_test_keras.json"}
}

print("Loading models, metrics, and confusion matrices from MLflow...")

# Create a temporary directory for downloading confusion matrix artifacts
temp_artifact_dir = "temp_mlflow_artifacts_cm_combined"
os.makedirs(temp_artifact_dir, exist_ok=True)


for model_name, info in model_info.items():
    print(f"\n\n  Обработка модели '{model_name}'...")
    # Get the latest version of the registered model
    latest_version_info = client.get_latest_versions(model_name, stages=["None"])[0]
    latest_version = latest_version_info.version
    run_id = latest_version_info.run_id
    artifact_path = info["artifact_name"]
    cm_artifact_name = info["cm_artifact_name"]

    # Load models
    model_uri = f"runs:/{run_id}/{artifact_path}"
    model = None

    if info["is_keras"]:
        model = mlflow.tensorflow.load_model(model_uri)
    else:
        if model_name == "XGBoost_Phishing_Final":
             model = mlflow.xgboost.load_model(model_uri)
        else:
             model = mlflow.sklearn.load_model(model_uri)

    loaded_models[model_name] = model
    print(f"    Model '{model_name}' successfully loaded.")

    # Collect metrics
    run = client.get_run(run_id)
    metrics = run.data.metrics

    test_roc_auc = metrics.get('test_roc_auc')
    test_precision = metrics.get('test_precision')
    test_recall = metrics.get('test_recall')
    test_f1 = metrics.get('test_f1')
    train_time_sec = metrics.get('train_time_sec')

    model_metrics[model_name] = {
        'test_roc_auc': test_roc_auc,
        'test_precision': test_precision,
        'test_recall': test_recall,
        'test_f1': test_f1,
        'train_time_sec': train_time_sec
    }
    print(f"    Metrics for '{model_name}' collected.")

    # Load confusion matrix artifact
    cm_artifact_uri = f"runs:/{run_id}/{cm_artifact_name}"
    local_cm_path = mlflow.artifacts.download_artifacts(
        artifact_uri=cm_artifact_uri,
        dst_path=temp_artifact_dir
    )

    print(f"    Confusion matrix artifact '{cm_artifact_name}' downloaded locally: {local_cm_path}")

    with open(local_cm_path, 'r') as f:
        cm_data = json.load(f)

    confusion_matrices[model_name] = np.array(cm_data)
    print(f"    Confusion matrix for '{model_name}' successfully loaded. Shape: {confusion_matrices[model_name].shape}")

print("\nLoading models, collecting metrics, and loading confusion matrices completed.")

"""5.2 ROC Curve Comparison"""

# Dictionary to store ROC curve data
roc_data = {}

print("Calculating ROC curves for models:")

# Calculate ROC curve for each model
for model_name, model in loaded_models.items():
    try:
        print(f"\n  Calculating for model '{model_name}'...")

        # Get predicted probabilities for the positive class on the test set
        if model_name == "Keras_Phishing_Final":
             y_pred_proba = model.predict(X_test_scaled)[:, 0]
        else:
             y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]

        # Calculate FPR, TPR, and thresholds
        fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)

        # Calculate AUC
        roc_auc = auc(fpr, tpr)

        # Save data
        roc_data[model_name] = {
            'fpr': fpr,
            'tpr': tpr,
            'auc': roc_auc
        }
        print(f"    Calculation for '{model_name}' completed. AUC: {roc_auc:.4f}")

    except Exception as e:
        print(f"  Error calculating ROC curve for model '{model_name}': {e}")
        roc_data[model_name] = None

print("\nROC curve calculation completed.")

# Visualize ROC curves
print("Visualizing ROC curves:")

fig = go.Figure()

# Add ROC curve for each model
for model_name, data in roc_data.items():
    if data is not None:
        fig.add_trace(go.Scatter(x=data['fpr'], y=data['tpr'],
                                 mode='lines',
                                 name=f'{model_name} (AUC = {data["auc"]:.4f})'))

# Add random classifier line (AUC = 0.5)
fig.add_shape(
    type='line', line=dict(dash='dash'),
    x0=0, x1=1, y0=0, y1=1
)

fig.update_layout(
    title="ROC Curve Comparison",
    xaxis_title="False Positive Rate (FPR)",
    yaxis_title="True Positive Rate (TPR)",
    xaxis=dict(range=[0, 1]),
    yaxis=dict(range=[0, 1]),
    legend=dict(
        orientation="h",
        yanchor="bottom",
        y=1.02,
        xanchor="right",
        x=1
    )
)

fig.show()

print("\nROC curve visualization completed.")

"""5.3 Grouped Bar Chart of Metrics: **Precision, Recall, F1**"""

# Create DataFrame from collected metrics
metrics_df = pd.DataFrame.from_dict(model_metrics, orient='index')
metrics_df = metrics_df.rename(columns={
    'test_roc_auc': 'Test ROC AUC',
    'test_precision': 'Test Precision',
    'test_recall': 'Test Recall',
    'test_f1': 'Test F1-score'
})

print("Visualizing metrics (Precision, Recall, F1-score):")

# List of metrics for plotting
metrics_to_plot = ['Test Precision', 'Test Recall', 'Test F1-score']

fig = go.Figure()

# Add columns for each metric per model
for metric_name in metrics_to_plot:
    fig.add_trace(go.Bar(
        x=metrics_df.index,
        y=metrics_df[metric_name],
        name=metric_name
    ))

# Update layout
fig.update_layout(
    title='Comparison of Precision, Recall, and F1-score on Test Set',
    xaxis_title='Model',
    yaxis_title='Metric Value',
    barmode='group',
    yaxis=dict(range=[0.85, 1.0]),
    legend=dict(
        orientation="h",
        yanchor="bottom",
        y=1.02,
        xanchor="right",
        x=1
    )
)

fig.show()

print("\nMetrics visualization completed.")

"""5.4 Confusion Matrix Visualization"""

# Labels for axes
labels = ['Legitimate (0)', 'Phishing (1)']

for model_name, cm in confusion_matrices.items():
    print(f"\n  Visualizing for model '{model_name}':")
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
    plt.title(f'Confusion Matrix: {model_name}')
    plt.xlabel('Predicted Class')
    plt.ylabel('True Class')
    plt.show()

print("\nConfusion matrix visualization completed.")

"""5.5 Summary Table: Overview of All Metrics"""

print("Building summary table of metrics:")
# Create DataFrame directly from model_metrics dictionary
summary_df = pd.DataFrame.from_dict(model_metrics, orient='index')

# Rename columns for better readability in the table
summary_df = summary_df.rename(columns={
    'test_roc_auc': 'ROC AUC',
    'test_precision': 'Precision',
    'test_recall': 'Recall',
    'test_f1': 'F1-score',
    'train_time_sec': 'Training Time (sec)'
})

# Format metric columns
for col in ['Test ROC AUC', 'Test Precision', 'Test Recall', 'Test F1-score']:
    if col in summary_df.columns:
        summary_df[col] = summary_df[col].apply(lambda x: '{:.4f}'.format(x) if x is not None else 'N/A')

if 'Train Time (sec)' in summary_df.columns:
    summary_df['Train Time (sec)'] = summary_df['Train Time (sec)'].apply(lambda x: '{:.2f}'.format(x) if x is not None else 'N/A')


# Print the summary table
print("\nSummary table of metrics and training time:")
display(summary_df)
